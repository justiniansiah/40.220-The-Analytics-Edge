---
title: "Hitters Notebook"
output: html_notebook
---

The hitters dataset consists of 322 observations of 21 variables with the following information - X (name), AtBat, Hits, HmRun (home runs), Runs, RBI, Walks, Years, CAtBat, CHits, CHmRun, CRuns, CRBI, CWalks, League, Division, PutOuts, Assists, Errors, Salary, New League. Here League, Division and NewLeagues are factor variabes with 2 categories. We drop rows with missing entries and are left with 263 observations.
```{r}
hitters <- read.csv("hitters.csv")
str(hitters)
hitters<-na.omit(hitters)
str(hitters)
```
The leaps package in R does subset selection with the regsubsets function. By default, the maximum number of subsets, this function uses is 8. We extend this to do a complete subset selection by changing the default value of nvmax argument in this function. Note that CRBI is in the model with 1 to 6 variables but not in the model with 7 and 8 variables.
```{r}
install.packages("leaps")
library(leaps)
?regsubsets
hitters <- hitters[,2:21]
model1 <- regsubsets(Salary~.,hitters)
summary(model1)
model2<-regsubsets(Salary~.,hitters,nvmax=19)
summary(model2)
```



```{r}
names(summary(model2))
summary(model2)$rsq
plot(summary(model2)$rsq)
plot(summary(model2)$rss)
plot(summary(model2)$adjr2)
which.max(summary(model2)$adjr2)
coef(model2,11)
```
The figures indicate that R-squared increase as the number of variables in the subset increases and likewise the residual sum of squared (sum of squared errors) decreases as the size of the subsets increases. On the other hand the adjusted R-squared increases first and then decreases.

Forward stepwise selection: In this example, the best model identified by the forward stepwise selection is the same as that obtained by the best subset selection. It is also possible to run this algorithm using a backward method where you drop variables one a time rather add. In general, the solutions from these two methods can be different.
```{r}
model3<-regsubsets(Salary~.,data=hitters,nvmax=19,method="forward")
which.max(summary(model3)$adjr2)
coef(model3,11)
summary(model2)$adjr2-summary(model3)$adjr2
plot(summary(model3)$adjr2)
```

LASSO: The generalized linear model with penalized maximum likelihood package glmnet in R implements the LASSO method. To run the glmnet() function, we need to pass in the arguments as X (input matrix), y (output vector), rather than the y~X format that we used thus far. The model.matrix() function produces a matrix corresponding to the 19 predictors and the intercept and helps transform qualitative variables into dummy quantitative variables. This is important since glmnet() works only with quantitative variabes.
```{r}
install.packages("glmnet")
library(glmnet)
X <- model.matrix(Salary~.,hitters)
y <- hitters$Salary
str(X)
```
We now choose a range for the lambda parameters and create a training and test set. We then build the LASSO model on this data. The output from the model provides the Df (non of nonzeros), %Dev and Lambda values. The deviance measure is given as 2(loglike_sat - loglike), where loglike_sat is the log-likelihood for the saturated model (a model with a free parameter per observation). Null deviance is defined to be
2(loglike_sat - loglike(NULL)) where the NULL model refers to the intercept model only. The deviance ratio is dev.ratio=1-deviance/nulldev. As lambda decreases, the dev.ratio increases (more importance given to model fit than model complexity).
```{r}
grid<-10^seq(10,-2, length=100)
set.seed(1)
train <- sample(1:nrow(X),nrow(X)/2)
test <- -train

modellasso <- glmnet(X[train,],y[train],lambda=grid)
summary(modellasso)
modellasso
deviance(modellasso)
plot(modellasso,xvar="lambda",label=TRUE)
```
We see from the plot that as lambda increases, many of the coefficients get close to zero. We can retrieve these coefficients as follows. Note that the number of non-zero coefficients does not change in a fully  monotonic way, as lambda increases or decreases.
```{r}
modellasso$df
modellasso$beta
coef(modellasso)
```
Predictions: We start with a prediction for the model fitted with lambda = 100. The test mean squared error for this model is 126478. Suppose, we change lambda to 50, we get 105908 and if we change lambda to 200, we get 177294. Note that by default if prediction is done at lambda values that are not tried in the fitting algorithm, it uses linear interpolation to make predictions. We can use exact=T in the argument to get the exact value by refitting. In addition, you need to then pass also the original training set data to the function. You get a test error of 115096 with the full model while 193253 with a very large value of lambda. Thus choosing lambda appropriately will be important in the quality of the fit. This can be done with cross-validation.
```{r}
modellasso$lambda
predictlasso1 <- predict(modellasso,newx=X[test,],s=100)
mean((predictlasso1-y[test])^2)
predictlasso2 <- predict(modellasso,newx=X[test,],s=50)
mean((predictlasso2-y[test])^2)
predictlasso3 <- predict(modellasso,newx=X[test,],s=200)
mean((predictlasso3-y[test])^2)
?predict.glmnet
predictlasso1a <- predict(modellasso,newx=X[test,],s=100,exact=T,x=X[train,],y=y[train])
mean((predictlasso1a-y[test])^2)
predictlasso2a <- predict(modellasso,newx=X[test,],s=50,exact=T,x=X[train,],y=y[train])
mean((predictlasso2a-y[test])^2)
predictlasso3a <- predict(modellasso,newx=X[test,],s=200,exact=T,x=X[train,],y=y[train])
mean((predictlasso3a-y[test])^2)
predictlasso4 <- predict(modellasso,newx=X[test,],s=0,exact=T,x=X[train,],y=y[train])
mean((predictlasso4-y[test])^2)
predictlasso5 <- predict(modellasso,newx=X[test,],s=10^10,exact=T,x=X[train,],y=y[train])
mean((predictlasso5-y[test])^2)
```
Cross-validation: By default, you perform 10 fold cross validation. Note that glmnet uses randomization in choosing the folds which we should be able to control better by setting the seed to be the same. The optimal value of lambda found from cross validation is 22.18. You can plot the lambda parameter with the cross-validated mean error (cvm). We see that the best fit from model with the optimal lambda gives a much smaller error on the test set than the model which is based on complete linear regression or the model with only an intercept. We can print out the coefficients to identify that 7 variables are chosen (excluding the intercept).
```{r}
set.seed(2)
cvlasso <- cv.glmnet(X[train,],y[train])
cvlasso$glmnet.fit
cvlasso$lambda.min
plot(cvlasso$lambda,cvlasso$cvm)
predictlassocv <- predict(modellasso,s=22.18,newx=X[test,])
mean((predictlassocv-y[test])^2)
coef(modellasso,s=22.18)
```