
# Script for ...


######## 0. Set the working environemnt

# Remove all variables from the R environment to create a fresh start
rm(list=ls())

# Set the working folder
# getwd()
setwd("D:/School Stuff/40 .220 The Analytics Edge/W10/1) Recommendation Systems")

# Load the ratings dataset
ratings <- read.csv("ratings.csv")
str(ratings)
length(unique(ratings$userId))  # 706
length(unique(ratings$movieId)) # 8552
sort(unique(ratings$rating))    # 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
# The ratings dataframe consists of 100,023 observations of 3 variables (userId, movieId, and rating)

# Users have rated from 20 to 2268 movies
max(table(ratings$userId)) # 2268
min(table(ratings$userId)) # 20
# Here is the list of users that have watched 2268 and 20 movies
which(table(ratings$userId)==2268)
which(table(ratings$userId)==20)
# Movies have from 1 to 337 ratings
max(table(ratings$movieId))
min(table(ratings$movieId))


######## 1. We can start preparing the data for the analysis

# Create an empty matrix with 706 rows (users) and 8552 (movies)
Data <- matrix(nrow=length(unique(ratings$userId)), ncol=length(unique(ratings$movieId)))
# We name the rows and columns with the unique users and movieid in the dataset
rownames(Data) <- unique(ratings$userId)
colnames(Data) <- unique(ratings$movieId)

# We can now fill in the matrix
for(i in 1:nrow(ratings)){
  Data[as.character(ratings$userId[i]),as.character(ratings$movieId[i])] <- ratings$rating[i]
}
dim(Data)     # 706 8552
# Visual check
hist(as.vector(Data))

# Normalize the ratings of each user so that bias is reduced (na.rm=TRUE omitts the missing values)
Datanorm <- Data - rowMeans(Data,na.rm=TRUE)
# Visual check --> we have a more symmetric rating when using normalized data
hist(as.vector(Datanorm))
 
# Split the data 
# We want to create a matrix with spl1 + spl1c rows and spl2 + spl2c columns
set.seed(1)       
spl1 <- sample(1:nrow(Data), 0.98*nrow(Data)) # spl1 has 98% of the rows (users)
spl1c <- setdiff(1:nrow(Data),spl1)           # spl1c has the remaining ones
set.seed(2)
spl2 <- sample(1:ncol(Data), 0.8*ncol(Data))  # spl2 has 80% of the columns (movies)
spl2c <- setdiff(1:ncol(Data),spl2)           # spl2c has the rest
length(spl1)  # 691
length(spl1c) # 15
length(spl2)  # 6841
length(spl2c) # 1711


######## 2. Models

# We create three different prediction models and corresponding predicted rating matrices

# First, we initialize the matrices for the three models
Base1    <- matrix(nrow=length(spl1c), ncol=length(spl2c))
Base2    <- matrix(nrow=length(spl1c), ncol=length(spl2c))
UserPred <- matrix(nrow=length(spl1c), ncol=length(spl2c))

# Then, we make the predictions for models Base1 and Base2
#
# Base1: average out the item rating for all users in the training set (spl1) [average item rating]
#for all movies of interest(the 15 in spl1c)
for(i in 1:length(spl1c)){
  Base1[i,] <- colMeans(Data[spl1,spl2c], na.rm=TRUE)  #mean of each movie across all users who already rated (spl1)
}
Base1[,1:5] # All rows (users) contain the same information
#
# Base2: average over the user rating for all the items in the training set (spl2) [average user rating]
for(j in 1:length(spl2c)){
  Base2[,j] <- rowMeans(Data[spl1c,spl2], na.rm=TRUE) #mean of all the rated users' movies 
}
Base2[,1:5] # All columns (movies) contain the same information

# For the third model, we first need to calculate the correlation between users
#
# Initialize matrices
Cor <- matrix(nrow=length(spl1),ncol=1) # keeps track of the correlation between users
Order <- matrix(nrow=length(spl1c), ncol=length(spl1)) # sort users in term of decreasing correlations
#
# With this command, we create an Order matrix of dimensions 15x691, 
# where each row corresponds to neighbors of the users in the spl1c in decreasing order of Pearson correlation. 
# The NA accounts for users who have no common ratings of movies with the user.
#compare all 'new' users
for(i in 1:length(spl1c)){
  #to all 'known' users
  for(j in 1:length(spl1)){
      Cor[j] <- cor(Data[spl1c[i],spl2],Data[spl1[j],spl2],use = "pairwise.complete.obs") #correlation btwn 'new' user iand every 'known' user
      }
  V <- order(Cor, decreasing=TRUE, na.last=NA) #arrange in decreasing order (largerst cor to smallest)
  Order[i,] <- c(V, rep(NA, times=length(spl1)-length(V)))  #store 'new' user i's cor data in order matrix
}
# Let's check the results
length(Cor)
dim(Order)
Order[,1:20]
Order[,500:520]
Order[,671:691]
#
# Now, we compute user predictions by looking at the 250 nearest neighbours 
# and averaging equally over all these user ratings in the items in spl2c
# We can take the 1st 250 because we already ordered then previously
for(i in 1:length(spl1c)){
  UserPred[i,] <- colMeans(Data[spl1[Order[i,1:250]],spl2c], na.rm=TRUE)
}    


######## 3. Compare the model performance

# Calculate the model error
RMSEBase1    <- sqrt(mean((Data[spl1c,spl2c] - Base1)^2, na.rm=TRUE)) # 0.9310296
RMSEBase2    <- sqrt(mean((Data[spl1c,spl2c] - Base2)^2, na.rm= TRUE)) # 0.9953377
RMSEUserPred <- sqrt(mean((Data[spl1c,spl2c] - UserPred)^2,na.rm=TRUE)) # 0.8985567

# Last step: we vary the neighborhood set for the third model to see whether it positively affects the results
RMSE <- rep(NA, times=490)
for(k in 10:499){
  for(i in 1:length(spl1c)){
    UserPred[i,] <- colMeans(Data[spl1[Order[i,1:k]],spl2c], na.rm=TRUE)
  }
  RMSE[k-10] <- sqrt(mean((Data[spl1c,spl2c] - UserPred)^2,na.rm=TRUE))
}
plot(10:499,RMSE)
abline(h=0.931)







